---
title: "爬虫及文本挖掘"
author: "胡帆"
date: "2017年4月27日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE}
library(stringr)
library(rvest)
library(httr)
library(jiebaR)
library(tm)
library(wordcloud2)
library(ggplot2)
```

# 1. 爬虫

爬虫，在这里指从网页上抓取数据。

爬取数据要经过以下几个步骤：

1. 了解网页结构基本知识

2. 学会使用rvest包抓取静态网页

3. 学会使用httr包抓取动态加载的网页

4. 清理抓下来的数据

5. 存入本地csv文件或数据库中

## 1.1 HTTP 协议介绍

HTTP全称是 Hyper Text Transfer Protocol，即超文本传输**协议**，但是HTTP并不仅仅用来传输超文本的标准，它还可以被用来向服务器请求几乎任何类型的资源。

![](C:/Users/hufan/Documents/研一下/WiseRClub/http协议.jpg)

 1. 用户在浏览器里输入想要到达的 URL
 
 2. 浏览器起到http客户端的作用，用来发起请求
 
 3. http客户端在DNS域名系统中查找机器能读懂的URL对应的IP地址
 
 4. 通过TCP/IP协议，http客户端连接到http服务器
 
 5. http服务器响应请求，返回用户所需要的文件
 
 6. 所有请求文件发送完毕后，服务器会关闭与http客户端的连接
 

### 1.1.1 URL

全称是Uniform Resource Locators，翻译过来是统一资源定位符，anyway，我们不用管它的官方翻译是什么意思，它其实就是我们日常说的网址。我们只需要了解它的格式:

        scheme://hostname:port/path?querystring#fragment
        
对应过来是：
 
 - scheme：传输协议，对应HTTP、HTTPS、FTP、SMTP等
 
 - hostname：主机名，是我们想要发送请求的目的地，它是服务器的名字。比如`www.google.com`,也可以是DNS中查找到的对应IP地址。
 
 - port：HTTP缺省端口为 80，用来传输控制协议（TCP）；HTTP缺省端口是443，建立到服务器的TCP连接。一般浏览器会默认使用缺省端口80，所以URL中这一项**可以省略**。
 
 - path：这是你要请求获取的资源所在服务器的位置。类似于电脑本地某个文件的文件路径。例如 `https://github.com/hadley/rvest`中的 `/hadley/rvest` 就是资源所在服务器的路径。
 
 - ?querystring：当你有查询需求时，这用来告诉服务器你的查询内容是什么。其形式是：`?name=value`，当有多个查询语句时，会使用`&`隔开。例如`https://github.com/search?q=httr`里的`?q=httr`，其表明你请求查询有关于`httr`的内容。
        
        
        
- reference:

   [统一资源定位符](https://zh.wikipedia.org/wiki/%E7%BB%9F%E4%B8%80%E8%B5%84%E6%BA%90%E5%AE%9A%E4%BD%8D%E7%AC%A6)

### 1.1.2 HTTP 消息

HTTP 消息，指的是浏览器的请求消息，以及服务器端响应的消息。Anyway，它由三部分组成：

 1. start line（起始行）
 
 2. headers（标头）
 
 3. body（正文）
 
#### 1.1.2.1 起始行

- 请求模式（request mode）下的格式如下：

<div align="center">请求方法  /请求资源   浏览器能处理的HTTP最高版本</div>
<br/>

例如：`GET /index.html HTTP/1.1`，表示发送请求的方式是`GET`，请求的资源是`index.html`，浏览器能处理的HTTP最高版本是`1.1`。

<br>

- 响应模式下（response mode）下的起始行格式如下：

<div align="center">服务器能处理的HTTP最高版本  状态码  关于响应状态的人话解释</div>
<br/>
例如：`HTTP/1.1 200 OK`，表示服务器能处理的HTTP最高版本是`1.1`，状态码（[status code](https://zh.wikipedia.org/wiki/HTTP%E7%8A%B6%E6%80%81%E7%A0%81)）是`200`，状态码`2XX`表示响应成功，后面跟的`OK`，也是对状态码的解释。

**Note**：状态码是用来表示HTTP响应状态的3位代码，其范围为：100~599。所有状态码的第一个数字代表了响应的 5 种状态之一。常见的状态码有 200，表示响应成功，以及众所周知的 404，表示 Page Not Found...

#### 1.1.2.2 标头

- 请求状态下的标头

有时候为了帮助服务器理解你的请求，我们需要给服务器端提供一些额外的信息，这时候我们就需要headers，即标头，可以用来指定我们希望服务器返回资源的语言类型及数据类型（文本类型或二进制类型）。这样服务器端通过读取标头信息，就能返回我们想要的正确形式的内容。

- 响应状态下的标头

我们主要关心标头里的cookies，这里的cookies是服务器端在返回响应数据时放在响应标头里的一些数据。

- 响应标头里cookies的作用

- 什么是cookie？

简单来说，cookie是用来进行用户身份识别的一个工具，通俗地说，我得知道你在发送第二次请求时，还是不是上次那个用户。服务器端为了更好地了解发出请求的用户，会设定一个唯一的标识ID和其他的一些数据，通过存放在响应标头中，返回给用户，然后用户端会把标头里的cookie存放在本地的一个文本文件中（这样子，本地cookies中会存放许多cookie，他们是用户端向不同服务器端发送过请求的，然后服务器响应标头中的cookie）。等到下次用户发送请求时，用户端（浏览器）会自动搜索本地保存的cookies，看其中是否有属于该服务器的cookie，有则把对应的cookie加入到请求中。

例如，你登录了一个网站，并发送了一次你的请求，当你发送第二次请求时，你并没有被要求再次登录，这是为什么？就是因为第一次请求得到响应后，服务器端发给你的cookies在起作用。


- cookie的分类

    1. Session cookie：会话cookie，在你关闭浏览器的时候，cookie也会被随之删除
    
    2. persistent cookie：持久性cookie，设定cookie的生命期，设置生命期有两个参数：max-age和expires，分别表示保存cookie的最长时间、终止日期。没有到生命终点，cookie便会一直存在，无论你是否关闭浏览器。
    
    3. third-party cookie：第三方cookie，日常浏览网站时出现的页面广告便是第三方cookie作祟，广告商服务器端通过收集你的信息，定制个性化广告，在你浏览其他网站时，将广告嵌入该网站，于是就出现了你十分厌恶的广告，浏览的网站只能说：“这个锅我不背啊啊啊啊”。
    
    你可能会因此对cookie的应用产生怀疑，虽然它会偷看你浏览的内容，但是不管怎么说，cookie被设计出来的初衷是好的啊！


#### 1.1.2.3 正文

- 请求状态下的正文

请求状态下的正文可以省略。

- 响应状态下的正文

响应状态下传回的正文，包含了用户请求的数据资源。**我们使用httr动态抓取网页时，就是要找它。**

### 1.1.3 请求方法

- GET 方法和 POST 方法的区别

举例来说：

假如我现在有一句话想告诉你，我可以直接在大庭广众下写出来给你看，这是GET方法。

但某些情况下，如果我觉得这句话很私密，不想直接在大庭广众下写出来给你看，那么我会把它写在一个只有你能打开的日记本里，然后我把日记本交给你，让你来打开日记本，看到我写的那句话。这就是POST方法。
更形象地说，POST 方法发送请求的过程，就相当于我把请求数据放在 CSV 文件里，而你需要用 EXCEL 来解析这个 CSV 文件，才能获取我的请求数据。

[如果你想好好了解HTTP，戳这里](http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386832653051fd44e44e4f9e4ed08f3e5a5ab550358d000)

### 1.1.4 爬虫利器

- chrome Web Development tool 谷歌开发者工具

   + 关注`element`和`NETWORK`面板

- firefox

- SelectorGadget 自动帮你寻找定位标签的最优路径


### 1.2 Rvest 包入门

```{r}
#install.packages("rvest")
library(rvest)
single_table_page <- read_html("./case/single-table.html")
```
- Rvest包中常用函数一览：

| 函数|作用|
|------------------------|-------------------------------------------------------------|
|read_html() |读取 html 页面|
|html_nodes() |提取所有符合条件的节点|
|html_node() |返回一个变量长度相等的list，相当于对html_nodes()取[[1]]操作|
|html_table()|获取 table 标签中的表格,默认参数trim=T,设置header=T可以包含表头，返回数据框|
|html_text() |提前标签包含的文本,令参数trim=T,可以去除首尾的空格|
|html_attrs(nodes)|提取指定节点所有属性及其对应的属性值，返回list|
|html_attr(nodes,attr)|提取节点某个属性的属性值|
|html_children()|提取某个节点的孩子节点|
|html_session()|创建会话|

- css选择器与xpath用法对比

css选择器和xpath方法都是用来定位DOM树的标签（DOM指html内容），只不过两者的定位方式上存在一些小差别：

|目标	|CSS 3	|XPath|
|-----------------------|---------------|-------------------------------------|
|所有元素| 	`*` |	`//*`|
|所有的P元素 |	`p` |	`//p`|
|所有的p元素的子元素 |`p > *` |	`//p/*`|
|根据ID获取元素 |	`#foo` |	`//*[@id='foo']`|
|根据Class获取元素 |	`.foo`      |  ` //*[contains(@class,'foo')]` |
|拥有某个属性的元素 |	`*[title]` |	`//*[@title]`|
|所有P元素的第一个子元素 |	`p > *:first-child` 	|`//p/*[0]`
|所有拥有子元素a的P元素|无法实现 	|`//p[a]`|
|下一个兄弟元素| 	`p + *`|	`//p/following-sibling::*[0]`|

#### 1.2.1 CSS 方法提取节点
```{r}
html_table(single_table_page)
html_table(html_node(single_table_page,"table"))
products_page <- read_html("./case/products.html")
products_page %>% html_nodes(".product-list li .name") %>% html_text() 
product_items <- products_page %>% html_nodes(".product-list li")
products <- data.frame(name=html_text(html_nodes(product_items,".name")),
                       price = as.numeric(gsub(x=html_text(html_nodes(product_items,".price")),"$","",fixed=TRUE)),
                       stringsAsFactors = FALSE)
```


#### 1.2.2 XPath 方法提取节点
```{r}
page <- read_html("./case/new-products.html")
```

在Xpath里，`//`表示在上一节点的所有后代节点中搜索该标签，`/`表示只在上一节点的孩子节点中搜索该标签

```{r, eval=FALSE, include=FALSE}
page %>% html_nodes(xpath="//p")
#CSS's way
page %>% html_nodes("p")
```

```{r, eval=FALSE, include=FALSE}
# 找到所有具有class属性的li标签
page %>% html_nodes(xpath="//li[@class]")
#CSS's way
page %>% html_nodes("li[class]")
```


```{r, eval=FALSE, include=FALSE}
# 找到id=‘list’的div标签下的所有li标签
page %>% html_nodes(xpath="//div[@id='list']/ul/li")
#CSS's way
page %>% html_nodes("div#list > ul > li")
```

查询后代节点既可以用xpath方法实现，也可以用css方法实现

但查询前继节点，我们只能用xpath方法实现。
```{r, eval=FALSE, include=FALSE}
page %>% html_nodes(xpath="//div[p]")
page %>% html_nodes(xpath = "//span[@class='info-value' and text()='Good']")
page %>% html_nodes(xpath="//li[div/ul/li[1]/span[@class='info-value' and text()='Good']]/span[@class='name']")
page %>% html_nodes(xpath="//li[div/ul/li[2]/span[@class='info-value' and text()>3]]/span[@class='name']")
```


#### 案例一：爬R包网页

- 抓取网页数据
```{r}
page <- read_html("https://cran.rstudio.com/web/packages/available_packages_by_name.html")
#pkg_table <- page %>% html_table(fill=TRUE) # 返回list，所包含的数据在第一个list中
#class(pkg_table)
pkg_table <- page %>% html_node('table') %>% html_table(fill=TRUE) #返回数据框
# 由于原表格没有表头，因此数据框使用默认的表头代替
# 使用fill=T,自动填补第一行的缺失值
class(pkg_table)
head(pkg_table,5)
```



- 数据清理
```{r}
# 删除缺失值
pkg_table <- pkg_table[complete.cases(pkg_table),]
# 定义表头
colnames(pkg_table) <- c("name","title")
head(pkg_table,3)
```

#### 案例二：爬取 StackOverFlow 上有关于 R 的问题

给定起始页面以及爬取页数，要求得到每一个问题的标题、票数、回答数、查看数，并把这些问题的信息拼接成一个数据框：
```{r}
QInSTF <- function(url,page){
        #QInSTF denotes the relative info of R questions in the stackoverflow
        # page denotes the number of question_page you want to search
        require(rvest)
        t <- data.frame()
        for(i in 1:page){
                i=1
                u <- paste(url,"r?page=",as.character(i),"&sort=votes&pagesize=15",sep="")
                html <- read_html(u)
                df <- list(title = html %>% html_nodes(xpath="//div[@class='summary']/h3") %>% html_text(),
                           vote = html %>% html_nodes(".vote-count-post") %>% html_text() %>% as.numeric(),
                           answer = html %>% html_nodes(xpath="//div[@class='stats']/div[2]/strong") %>% html_text() %>% as.numeric(),
                           views = html %>% html_nodes(xpath="//div[@class='statscontainer']/div[3]") %>% html_attr("title") %>% str_extract_all(pattern="[\\d\\,]+") %>% str_replace_all(pattern="\\,+",replacement="") %>% as.numeric()
                             )
                t <- rbind(t,df)
        }
        return(t)
}
url <- "http://stackoverflow.com/questions/tagged/"
df <- QInSTF(url,10)
View(df)
```


实际上你也可以马上得到 Python 的相关问题，只需要将链接中的r改成python：
```{r, eval=FALSE, include=FALSE}
u <- paste(url,"python?page=",as.character(i),"&sort=votes&pagesize=15",sep="")
```

-查找标签神器：SelectorGadget

人为地查找节点是比较麻烦的，那么下面我要说的神器就能大大提升我们的工作效率，给大家推荐一款自动搜寻最短路径的节点搜寻软件：[SelectorGadget](http://selectorgadget.com/)

-使用方法简要说明

1. 它虽然作为chrome的插件，但是用firefox的童鞋也不要灰心丧气，你只需要把SelectorGadget的指定链接拉入你的收藏夹，等到下次要查找节点的时候，点击收藏夹里的它，接下来的操作就和在chrome里装了插件一样。


2. 安装完毕后，先点击插件，然后选中你想要查找的网页内容，那么该内容会标绿。整个页面可能也会出现标黄的内容，而这些黄色部分表示的是为匹配同一节点的内容。重复点击某一内容，该内容会被标红，这表示排除刚刚重复点击的内容。

另外，hadley大神为我们写好了该插件的应用案例，只需要在R中输入以下代码：

```{r}
#vignette("selectorgadget")
```




#### 案例三：抓取百度百科的信息

抓取花儿与少年的百度百科中成员信息：

- 抓取网页数据 
```{r}
url <- "http://baike.baidu.com/item/%E8%8A%B1%E5%84%BF%E4%B8%8E%E5%B0%91%E5%B9%B4/13572794"
page <- read_html(url)
tables <- page %>% html_nodes("table[log-set-param=table_view]") %>% html_table()
length(tables)
View(tables[3])
table <- tables[3]
str(table)
```

- 数据清洗

我们只需要对成员介绍这一列进行数据请里，整理成我们想要的这种形式：

![想要的数据格式](C:/Users/hufan/Documents/研一下/WiseRClub/想要的数据格式.jpg)

<br>

1. 分割句子

将成员介绍那一列的每一个元素，均分割成我们想要的句子形式：

   姓名、代表作、个人描述
   
这三部分。

```{r}
t1 <- str_extract_all(string=table[[1]]$"成员介绍","[\\w[:punct:]]+")
```


2. 去除没有用的引用符号及其中的数字
```{r}
t2 <- lapply(t1,str_replace_all,pattern="[代表作：\\[\\d+\\]]",replacement="")
```

3. 去除空元素
```{r}
t3 <- lapply(t2,function(x) t(x[x!=""])) 
str(t3)
```
Note：这里讲列表中的列向量必须转化为行向量，才能在第5步中，使每一条成员信息都按行正确地拼接。

4. 将列表格式转化为数据框格式，便于下一步的拼接

要使用到`plyr`包里的`rbind.fill()`函数，这个函数只能接受数据框

```{r}
library(plyr)
t4 <- lapply(t3,as.data.frame)
dat <- Reduce(rbind.fill,t4)
names(dat) <- c("姓名","职业","代表作","个人描述")
```


5. 将转换完的数据与原来的“参与季度”那一列进行一个拼接。
```{r}
dat$"参与季度" <- table[[1]]$"参与季度"
View(dat)
```


#### 案例四：穿越表单


我们需要创建一个会话窗口，与服务器端沟通。会话窗口的作用在于，服务器端认为你是活跃着的用户，用户端会自动跟踪cookie，有了服务器端传回来的cookie，这样就能保持请求不被拒绝。

换句话说，有了会话，服务器端会把你当成人类，而不是爬虫。

利用`html_session()`创建会话，其返回的结果和之前`read_html()`的返回结果差不多，都包含了html网页内容，因此我们可以直接在`html_session()`返回结果上执行查找结点等操作，当然，有了会话，我们还可以实现页面的跳转。

- 穿越学校网络教学平台的表单
```{r}
formurl <- "http://open.xmu.edu.cn/oauth2/authorize?client_id=1010&response_type=code"
session <- html_session(formurl) #创建会话
form <- html_form(session) #得到网页内的所有表单，以list形式返回
str(form)
form <- form[[1]] #提取我们想要的表单
UserName <- "15420161152145"
Password <- "password"
form <- set_values(form,'UserName'=UserName,'Password'=Password) #填写表单内容
out_url <- submit_form(session,form) #在会话中提交表单，实现表单穿越
out_url
```



## 1.3 httr包抓取动态网页

### 1.3.1 httr包简介

rvest包虽然非常简单，但是它在抓取动态网页方面却显得不足，特别是对异步加载（AJAX）的网页，所以我们需要hadley大神的另一个包：httr

什么是异步加载：

举个例子，有一条长微博，你点击了”更多“，但页面却没有发生跳转，网址也没有发生变化，这就是异步加载。动态网页如今基本都使用了异步加载技术。

httr 包里我们需要搞清楚的有两部分：

1. 请求

2. 响应

我们利用测试网站 http://httpbin.org/get ，来给大家作介绍。

这个网站会返回你发送过去的所有内容。就是你给他发送什么，他就原样响应给你什么。

#### 1.3.1.1 标头header

在爬虫中比较重要请求标头内容是`user-agent`,`Host`,`Referer`,`cookies`。

响应标头中较为重要的内容包括`content-type`,`content-encoding`,`set_cookies`,`location`。

#### 1.3.1.2 请求方式

请求方式包括`GET`,`POST`,`PUT`,`DELETE`,`PATCH`。常用的是`GET`,`POST`方法，因此本文仅对`GET`,`POST`这两种方法进行介绍。

##### get 方法

`GET`方法中最重要的是`headers`标头部分。

1. 在`GET`请求中增加查询键值对。实际其`URL`变成了：`http://httpbin.org/get?key1=value1&key2=value2`。
```{r, include=FALSE}
(r <- GET("http://httpbin.org/get",
         query=list(key1="value1",key2="value2"))) #构造查询
```

**NOTE**：构造查询请求时，我们会使用`GET`方法。从`HTTP`相关知识中，我们知道查询的格式其实就是键值对，在`Python`里我们可以使用字典，在`R`中，我们使用命名列表来实现我们要提交的格式设计。

2. 往请求标头里增加属性，我们使用`add_headers(name1=val1，name2=val2,...)`的形式。
```{r, eval=FALSE, include=FALSE}
r <- GET("http://httpbin.org/get",add_headers(Name="hufan",Referer="Local Rstudio"))
content(r)$headers
```

3. 在请求头中伪造`cookie`。我们使用`set_cookies(name=val)`来伪造用于身份识别的`cookie`。
  
```{r, include=FALSE}
r <- GET("http://httpbin.org/cookies",set_cookies("Mewant"="cookies",session_id="1"))
r
```

  一般在网络请求中，我们会用`POST`方法来传递不想被人看到的信息，比如你的应户名、密码。不像`GET`中把信息显露无疑地摆放在`URL`中，`POST`方法把它的信息放在`body`中，随着请求传输到服务器端，让服务器端打开`body`，读取你的信息。下面我们就来介绍`POST`方法。
  
##### POST方法

  在POST方法中，这三个部分：`status_line`,`headers`,`body`，都比较重要。
  
  我们对`status_line`中的`status_code`最感兴趣，因为它反映了我们的请求是否被接受，不被接受的话，又是因为什么而拒绝我们的请求。
  
  上面我们已经定制了`GET`方法中的`headers`，当然，`POST`方法中也可以像`GET`中那样定制请求头，我就不重复了。在这里，主要讲解`POST`方法中怎么构造自己想要的`body`。
  
```{r, include=FALSE}
url <- "http://httpbin.org/post"
body <- list(a=1,b=2,c=3)
r <- POST(url,body=body,encode="form")
```
**Note**:只有当`body`是**命名列表**时，我们才可以指定`encode`参数。

#### 案例一：使用httr包抓取薛之谦新歌《动物世界》评论

这里需要抓包
```{r}
library(httr)
#获取薛之谦《动物世界》的歌曲评论
url = "http://music.163.com/weapi/v1/resource/comments/R_SO_4_468517654?csrf_token="
#构造标头
header <- c("application/x-www-form-urlencoded",
            "gzip, deflate",
            "http://music.163.com/",
            "JSESSIONID-WYYY=fCenT4%2BZen2li%2BVkwqfyaY%5CkKqEkEQ4fyeSyaefGfaXiSAVAVdA6PjNVYr5aJ0YXMGxm%5CA7%2Beu4hzggnVBxn0FimYlYUZjWMffk%5CIaev58NoSZsX70r70%5Cvgd7%2B9zKCgQxAQoOfCx%2F%2Fu%2BR26w4xzNbVQFPw1Xfq4DoH2i5xeaBsio9vs%3A1491197296757; _iuqxldmzr_=32; _ntes_nnid=f374e8d9fd15170c19f5f75186cfbb09,1486989651115; _ntes_nuid=f374e8d9fd15170c19f5f75186cfbb09; __utma=94650624.720052715.1487140369.1491183317.1491195035.3; __utmz=94650624.1491183317.2.2.utmcsr=bing.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __oc_uuid=4bb1c5e0-f768-11e6-9bed-cb1b9907a69b; __utma=187553192.582371522.1487593809.1490937323.1490949328.4; __utmz=187553192.1488870253.2.2.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; mail_psc_fingerprint=e0e1c03421a56e6f74684197b00ff225; P_INFO=hufan2012muc@163.com|1490698236|0|mail163|00&99|fuj&1490232015&mail163#fuj&350100#10#0#0|157013&0||hufan2012muc@163.com; __utmc=94650624; __utmb=94650624.6.10.1491195035")
names(header) <- c("Content-Type","Accept-Encoding","Referer","Cookie")
#body中的参数
body=list("csrf_token"="",
          "params"="wR1XOWKPd+6GTp+9+I58JOkOtZu7ENR+MbMTCTjOyael7zrfyZ+rBgfe/fNbqYj6Nij4MoqUcUxaoYDEpXJ35llzPDcL4o0TGIi7s2bnNPTMsHMj3bnnKHVjuAEt11BBhSGugqOUckBg1saPHM3emvfqcd3XGdmOen1xbAe8SWF2eIxmAL0flxdGbeUMH1Go",
          "encSecKey"="43c0d93de59260793304403afd42665acabf08ecb286bef2ecf57f6b888f94001d8cda9ffbd2459a65e4f8834be7117b5b85d18182ed81052d625af3caa002922b817642780449b7df28ff3ef7608b38982f05ec2510054e9c522e85e9aebcc3643226ba864640635ff30fd3c92401f4f0405d598648e73f6cc43e849cc55e51")
#POST方法请求资源
r <- POST(url,body=body,add_headers(header),encode="form",verbose())
```


#### 案例二：调用歌词的api接口进行抓取

```{r, eval=FALSE, warning=FALSE, include=TRUE}

#scraping the list of songs and corresponding id
library(httr)
url <- "http://music.163.com/weapi/cloudsearch/get/web?csrf_token="
cookie <- "_ntes_nnid=484b435c46e2eb6b563113db20cc895e,1487593736409; _ntes_nuid=484b435c46e2eb6b563113db20cc895e; mail_psc_fingerprint=97d26ebf733b32a06c99cbb055035d71; P_INFO=hufan2012muc@163.com|1490232015|0|mail163|00&99|fuj&1489910143&mail163#fuj&350200#10#0#0|157013&0||hufan2012muc@163.com; __utma=187553192.270240397.1487593742.1490919761.1491190846.3; __utmz=187553192.1490919761.2.2.utmcsr=open.163.com|utmccn=(referral)|utmcmd=referral|utmcct=/special/opencourse/machinelearning.html; __oc_uuid=6d413bf0-f768-11e6-a15d-8b596b174d70; JSESSIONID-WYYY=88y0WzBV0mnR%5Crt2SH%5Cy65xUCAAzfjitOEgpoJ4V2n%2BpMpCY7Q1TGCfFwWm%5CPuMx44e5%2BzDVM26Nhmj4BqtwHSQhc6aPEy%2BUl6V92coSiX90hkPHNExOMI5OvZnUDRAhAlz5JX2YuiFJ%5CmaSrFE%5Cs7qq0qHbZe%2FM2hQdj02%2FpFHzu6QM%3A1491222158317; _iuqxldmzr_=32; __utma=94650624.489148324.1491190730.1491215027.1491219139.5; __utmb=94650624.28.10.1491219139; __utmc=94650624; __utmz=94650624.1491215027.4.2.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided)"
content.type <- "appliaction/x-www-form-urlencoded"
headers <- c("_ntes_nnid=484b435c46e2eb6b563113db20cc895e,1487593736409; _ntes_nuid=484b435c46e2eb6b563113db20cc895e; mail_psc_fingerprint=97d26ebf733b32a06c99cbb055035d71; P_INFO=hufan2012muc@163.com|1490232015|0|mail163|00&99|fuj&1489910143&mail163#fuj&350200#10#0#0|157013&0||hufan2012muc@163.com; __utma=187553192.270240397.1487593742.1490919761.1491190846.3; __utmz=187553192.1490919761.2.2.utmcsr=open.163.com|utmccn=(referral)|utmcmd=referral|utmcct=/special/opencourse/machinelearning.html; __oc_uuid=6d413bf0-f768-11e6-a15d-8b596b174d70; JSESSIONID-WYYY=88y0WzBV0mnR%5Crt2SH%5Cy65xUCAAzfjitOEgpoJ4V2n%2BpMpCY7Q1TGCfFwWm%5CPuMx44e5%2BzDVM26Nhmj4BqtwHSQhc6aPEy%2BUl6V92coSiX90hkPHNExOMI5OvZnUDRAhAlz5JX2YuiFJ%5CmaSrFE%5Cs7qq0qHbZe%2FM2hQdj02%2FpFHzu6QM%3A1491222158317; _iuqxldmzr_=32; __utma=94650624.489148324.1491190730.1491215027.1491219139.5; __utmb=94650624.28.10.1491219139; __utmc=94650624; __utmz=94650624.1491215027.4.2.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided)",
             "application/x-www-form-urlencoded",
             "http://music.163.com/")
names(headers) <- c("Cookie","Content-Type","Referer")
formData <- c("params=JW9FGKdC3sCm6lxe%2FCXZSxlZM7blJmowgK3t9tCvYLYNEslk0onVnXIBZAZwBOh7MUoqAX2yI2Iznckac%2F9IyVxZlFlCu9XvsUjk2QPo4Kd6B116952btb1igESbjpnZclwDAfLIY1asBcdWeWWF1NnEzPk%2FftHcSyjIO%2Fmcp%2F61XiPz%2BWMh3b10Is%2BL2Q%2F6Wr1fe7GK9e3shN1ds93KR6%2BN1Uo5bX1Aqj%2FfrDGbnICnHDaS9otQL2hcrbYS7Ut0t6CYh%2FwrxI1pprcz6Ob6%2Ffl0ft34KxwXaDRykzXYil8%3D&encSecKey=d2d764f457b020e99c80ff18d31e4051eac14c56c0d4406fd84490ae1d6596e4497b8ccd792ee3936fb2d7f5bbff9e1a69b6660689798bd024af7afe6cf1fcaceed53490ce9a143abd45256897462856f95a1f9d9563e978b3bbb8b649759be38faac93ef6fdf8fbd488de0696a9781eeabeeef694c2ddf9917b4a69be4ad689",
                "params=4jnh4TYUNNAuwKQ2sT3Y3S%2BE4%2Bsl79Xq0WVv3TEP66EQgYGSl2H5SDZTW6kRywTD2AVpjA5jTnzQAZblBNTFAxis8TjXBntGZA0w7th8h1N8R6oNUNeIJZyhjRvcF%2BnMdHTO8Ntj3Mxxfj93nF%2FKS7UlAMNdIQC%2BXyqa6FyoDmg%2BpXRq0J%2FrnF%2F81E%2BENQMnOMjr9dYwi2UObbeJehmJOVPGXce%2B7%2FRbQUQ%2FrRb8gIJneB0REa5LNxrAPDWQiUKwwpTsTfpBG7gIm0LK4LCMRlhvmoyiEqgD%2FPcVKRJ8zo0o7K9nPOiPbeQh4ervSI8H&encSecKey=7316edfe71581999fb0574abfa9533ab087aa06b141fd45f664cb309cbf23e4e8dc548b778933cebd4779208fb74060800d9f3fab4e356c1bb10e98ad921ae2354bccfbb1a5e49ee099000a15a5469180ee7e40f686eccd0360c00a753df05811348c569bf24836aa5781ae7f09ba5161b8691867298f49c2f0b111ad74c6a87",
                "params=qlJbVqwnSZPFGaMIejQb5g5hshRilrkAehPOS4%2FjKRGtqo40nGZI1NNqqH%2BMvNYbTxa%2FVXed%2ByjmCT98giDmMKSvJRZfvHpEbBnTBzJeefVULgqm%2BI3i3PNE2Av4CXoiwi7OLt23MbJhtuhzXSH%2BqvlMRXXcHeDJ4bgzoUrWaDqX%2B0nvBG29gtOrpz70HQCZmVnIMqQiIuCVI9wk48pWaE5ulli%2BRJBA0OC2djkjUMcjLhbiqoCBC46BnxLciUlOfb37Fo0%2BG0TxYVHzBd3dX7S2Ghd69SRGi%2FTFDrgswq4zz9ExlGT5Cy50BGUWtCxt&encSecKey=1eb0266d0a985c6f681fb8666148a3b5ab7e7891ad170a43a3d69926eda1d4e8ebf14f2e0bcc76d05af453a6619a7ffb27d2ceec80696ccae2695d5b169c1577798d3f7335b0689f017c0e32daf3ec7a7227fbd2a094485c6fe2385e3eaf9376a18e0b20814481199eba7fd75f7df5153ef6341627043dbaf1a5f2871327d5b6")

r <- lapply(formData,
            FUN=function(x) POST(url,add_headers(headers),body=x,encode="form"))
name.id <- lapply(r,function(xx){item <- content(xx);
                        data.frame(lapply(item$result$songs,function(x) x[c('name','id')]))})
name.id <- t(data.frame(name.id))
dat <- as.data.frame(matrix(name.id,ncol=2,byrow=T),stringsAsFactors = F)
colnames(dat) <- c("name","id")
dat <- dat[!duplicated(dat[1]),] #duplicate返回逻辑值
lyric_url <- paste("http://music.163.com/api/song/media?id=",dat$id,sep="")
#head(lyric_url)

#爬取歌词
library(rvest)
lyric <- lyric_url %>% lapply(function(x){x %>% read_html() %>% html_text() %>% str_replace_all(pattern="([:punct:]+)|([a-zA-Z\\d~]+)",replacement=" ") %>% str_replace_all(pattern=" +",replacement=" ") %>% str_trim()})

```

- 网易云api接口的一个Reference：
[网易云音乐常用API浅析](http://moonlib.com/606.html)

## 2. 中文分词

### 2.1 jiebaR 库简介

jiebaR 是一个处理中文分词的库，在中文分词准确度方面有着良好的表现。

jiebaR库非常容易上手，我们一共才需要掌握11个函数，而且这些函数大多用法相似，非常容易记忆，来看看到底是哪些：

1. worker()

2. segment()

3. tagging()

4. keywords()

5. simhash()

6. distance()

7. vector_tag()

8. vector_keywords()

9. vector_simhash()

10. vector_distance()

11. new_user_word()

掌握好这些函数，你就能游刃有余地处理中文分词了。

#### 函数讲解

- worker()

`worker()` 函数依据不同的模型参数，可以产生7种分词器，分别是：

|模型参数type|含义|模型参数type|含义|
|----|---------------------------------------------|----|-------------------------------------|
|"mp"|最大概率模型，<br>依据给定词典的词频进行分词，<br>词频越大，越容易被分割出来，<br>最简单的分词模型，<br>用得比较少|"hmm"|隐马尔科夫模型，<br>将前后相邻的两个字组成一个词，计算其出现的频率，<br>当频率超过一个阈值时，便将其作为一个词进行索引，<br>能有效提取未在词典中出现的词|
|"mix"|混合模型，<br>因为是结合mp、hmm模型进行分词，所以被称为混合模型<br>它是默认分词器|"query"|索引模型，<br>用得比较少|
|"tag"|分词并标记词性，<br>若有标记词性的需求，**可以**指定模型参数为"tag"，<br>也可以是指定为"mp"，"hmm"，"mix"，然后把分词器传入`tagging()`函数|"keywords"|提取关键词，<br>基于给定的[TF-IDF](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)语料库进行提取，<br>若有提取关键词的需求，**必须**指定模型参数为"keywords"|
|"simhash"|使用这个模型，将返回一个list，<br>这个list包含simhash value，<br>以及基于"keywords"模型分割出的关键词向量，<br>而simhash value可以被进一步用来计算[海明距离](http://www.lanceyan.com/tech/arch/simhash_hamming_distance_similarity.html)（文本相似度）,<br>jiebaR中用`distance()`函数来计算海明距离，但它其实就是根据simhash value得到的|||



## 3. 文本挖掘入门




DTM，是Document-Term Matrix的缩写，也就是我们常说的文档-词矩阵。

文档-词矩阵，是指以文档文件为行，以词为列，用词语在某个文档中出现的频数来填充矩阵。

为什么要构造这样的矩阵？很显然，因为这样的矩阵让词与文档的关系变得一目了然，得到DTM后，我们也就得到了文档的词向量，这便于以后进一步的分析与挖掘。

当然，你也可以构造成TDM（Term-Document Matrix），也就是词-文档矩阵。但这两者没有什么实质的区别，TDM不过是DTM的转置而已。

### 3.1 构建语料库

Q：什么是语料库？

A：语料库就相当于多个文档的一个集合。

读入所有文档，并构造成语料库的形式，这只是我们进行文本挖掘的第一步。根据文档集的不同形式，我们又需要用不同的函数完成语料库的构建。下面介绍三种构造方式：

#### 2.1.1 构造单个csv文件的语料库

对于csv文件的读取，我们有以下三个步骤：

1. 先用`read.csv()`或者`read.table()`读取本地的csv文件

2. 使用tm包中的`DataframeSource()`加载该csv，并保存为变量`Source`，待进一步将其转化为语料库

3. 使用tm包中的`Corpus()`函数转化第二步中的`Source`，成功构建语料库！

```{r, warning=FALSE}
csv <- read.table("./case/毒鸡汤.csv",header=T,sep=",",stringsAsFactors=F)
str(csv)
d.corpus <- Corpus(DataframeSource(csv),readerControl=list(language=NA))
inspect(d.corpus[[1]])
```
Note：因为我们读入的是中文文档，并且 tm 包可选定的语言中不包括中文，所以这里我们把语言设为NA


#### 3.1.2 构造单个非csv文件的语料库

构建非csv文件的语料库，其步骤与读取单个csv文件的步骤差不多。只是在这里，我们用的是`readLines`来读取本地文本文件：

1. 先用`readLines()`读取本地文本文件，尽量指定编码防止读取出错，若有空值存在，事先删除空值

**Note**：建议使用 [Notepad++](https://notepad-plus-plus.org/) 打开文件，查看文件编码。

2. 使用tm包中的`VectorSource()`来加载已读入的文件，并保存为变量`Source`待下一步转化为语料库

3. 使用tm包中的`Corpus()`函数对第二步中的`Source`进行转化。
```{r, warning=FALSE, include=FALSE}
text <- readLines("./case/lyric.txt",encoding="UTF-8")
#text <- lyric
head(text)
text <- text[text!=""] #删除空值
head(text)

d.corpus <- Corpus(VectorSource(text),readerControl = list(language=NA))
head(as.character(d.corpus)) 
#d.corpus <- Corpus(VectorSource(text),readerControl = list(language="english"))
```


#### 3.1.3 构造同一目录下多个文档的语料库

当多个文档放在一个同目录下，即放在同一个文件夹中。这里使用的加载函数为`DirSource()`.

假设我现在有一个文件夹，名称为“倚天屠龙记全本”。而这个文件夹里只存放了倚天屠龙记小说的三部分，现在我们来用这三个文档构造语料库：

1. 找到包含这三个文档的文件夹名称，并搞清楚文本文件的编码（注意，这里的三个文档格式及编码应该一致！）

2. 利用tm包的`DirSource()`函数来加载这些文档至R环境中，并需要指定文档编码！

3. 使用tm'包的`Corpus()`函数来完成目录语料库的构建
```{r}
#source <- DirSource("./case/倚天屠龙记全本",encoding="GB2312")
#d.corpus <- Corpus(source,readerControl=list(language=NA))
```
到此，文本语料库已经构建完成！

### 3.2 清洗数据

语料库虽然已经构建完成，但我们还需要对文本语料库进行数据清洗。在这里，我们使用tm包中的`tm_map()`函数便可以达到目标，你可以这样理解`tm_map()`：它就相当于base包的`lapply()`，只不过它返回的不是`list`类型，而依然是语料库形式。

`tm_map(x,FUN,...)`中的`x`是你待处理的语料库，`FUN`是对语料库进行处理的函数，`...`是`FUN`中的参数。

中文语料库的数据清洗步骤包括：

1. 移除标点符号
```{r}
d.corpus <- tm_map(d.corpus,removePunctuation)
```

2. 移除数字
```{r}
d.corpus <- tm_map(d.corpus,removeNumbers)
```

3. 移除英文字母
```{r}
d.corpus <- tm_map(d.corpus,str_replace_all,pattern="[a-zA-Z]+",replacement="")
```

4. 移除多余的空格
```{r}
d.corpus <- tm_map(d.corpus,stripWhitespace)
```

以上是中文语料库的一般清洗步骤，对于英文语料库，我们通常还要加入：

- 将语料库中的英语单词全都转换为小写单词（或者大写）
```{r}
#d.corpus_en <- tm_map(d.corpus,tolower) 
```


- 找到词根，并只保留词根，删除同一词根的英语单词

这里我们需要使用到tm包里的`stemDocument()`函数，但这个函数对`Snowballc`包有依赖，所以我们在使用前必须保证已经加载了`Snowballc`这个包。
```{r}
#install.packages("Snowballc")
#library(Snowballc)
#d.corpus_en <- tm_map(d.corpus_en,stemDocument)
```
Note：

   +`stemDocument()`可以提取单词的词根

   +`stemCompletion()`可以补全词根

- 删除停止词

停止词一般是指我们日常说得特别频繁的词，可能在某些场景几乎必不可少，但与其他词相比，它没有多大实际意义，对我们的分析并不会有什么帮助，比如“I”,"He","She",“a”，"is","that","of"，“in"等。

```{r}
#d.corpus_en <- tm_map(d.corpus_en,removeWords,stopwords("en"))
```


当然，中文中也有停止词，比如”我们“，“然而”，“吗”，“那个”，“在”等。但是我们先不在这部分对中文词的停止词作删除操作，我们将在用jiebaR对中文语料库分词的那部分，通过加载停用词词典，直接移除中文停止词。


## 3.3 使用jiebaR对中文分词
- Question：

为什么先对中文进行分词，再使用tm包构造DTM？

- Answer：

利用缺省的 reader 读入文档时，tm 是基于空格进行词分割的，所以tm对英文单词有很好的分割效果。

然而，如果文档为中文，tm 还是会以空格作为单词的分割符。因此基于tm分割词语的原理，这种分割对中文是不适用的。

- 解决办法：

为了能够处理中文，我们可以将jiebaR的分词器传入tm_map函数中，对语料库进行分词。
```{r, include=TRUE}
mix <- worker(stop_word = "./stop.UTF8") #读入停止词词典
mix$bylines=TRUE #设置为分行输出
#如果不进行分行输出处理，那么分词结果将全部转变为向量，最终失去了词向量的对应文档标识
#inspect(d.corpus[1:3])

d.corpus2 <- tm_map(d.corpus,segment,mix)
class(d.corpus2)
#inspect(d.corpus2[1:3])
```
好了，我们已经对语料库分好词，我们可以进行下一步操作了！



## 3.4 构造DTM或者TDM

在经过以上步骤后，我们可以开始构造文档-词矩阵（词-文档矩阵）了，通过使用tm包里的`DocumentTermMatrix()`（TermDocumentMatrix()）函数，我们可以轻松得到文档-词矩阵（词-文档矩阵）：
```{r}
dtm <- DocumentTermMatrix(d.corpus2)
dtm2 <- DocumentTermMatrix(d.corpus2,control=list(weighting=weightTfIdf))
#tdm <- TermDocumentMatrix(d.corpus2)
#inspect(dtm[1:3,1:10])
#View(inspect(dtm2[1:3,1:10]))
```
一条语句就构造完成DTM了！通过使用`inspect()`函数，我们还可以查看dtm矩阵的内容。上面的`inspect`语句查看的就是文档-词矩阵的1至3行、1至10列的子矩阵。

Note：`inspect()`会随机挑选显示项，并不会按文档索引顺序来呈现。

`DocumentTermMatrix()`函数的`control`参数中有一个设置`weighting`，表明以何种加权方式来填充DTM的矩阵值。默认设置为`control=list(weighting=weightTf)`，一共有四种加权方式：

   1. weightTf

   2. weightTfIdf

   3. weightBin

   4. WeightSMART

## 3.5 词云制作 

词云我们使用wordcloud2这个包。

### 3.5.1 按词频对词语进行排序

我们需要先计算出词频，再去构建word-freq数据框，最后才能实现词云
```{r}
m <- as.matrix(dtm)
freq <- colSums(m)
freq <- sort(freq,decreasing = TRUE) #对词频按降序排列
```



### 3.5.2 实现词云制作

```{r}
str(freq)
```

显然，freq这个向量是一个命名向量，保留了对应的词语作为名称，这正是我们想要的，因为我们乐意利用命名向量轻易构造出word-freq矩阵：
```{r}
words <- names(freq)

word_freq <- data.frame(words=words,freq=freq) #构建word-freq数据框

wordcloud2(word_freq,size=0.8,color="random-light",backgroundColor="grey")
#letterCloud(word_freq,word="薛之谦",wordSize=2,color="random-light")

#figPath <- "./case/薛之谦.jpg"
#wordcloud2(word_freq,figPath=figPath,size=1,color="random-light")
```
- Reference

[R包之tm：文本挖掘包](http://www.bagualu.net/wordpress/archives/6112)